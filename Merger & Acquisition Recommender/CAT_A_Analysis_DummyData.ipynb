{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stable-baselines3\n",
      "  Using cached stable_baselines3-2.7.1-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting shimmy\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3 shimmy gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.sparse import coo_matrix\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `dummy_working.xlsx` into the files tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"dummy_working.xlsx\")\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()   # gives structural summary of df\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "cat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "print(\"Categorical/Text columns:\", cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_pct = df.isnull().mean().sort_values(ascending=True) # sorted columns by percentage of missing values\n",
    "missing_pct[missing_pct > 0]\n",
    "plt.figure(figsize=(10, 8)) # Height adjusted for column names\n",
    "missing_pct.plot(kind=\"barh\")\n",
    "plt.title(\"Percentage of Missing Values by Column\")\n",
    "plt.xlabel(\"Missing proportion\")\n",
    "plt.ylabel(\"Column Name\")\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7) # Grid on x-axis only\n",
    "plt.tight_layout() # Ensures labels aren't cut off\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between `Revenue (USD)` and `Market Value (USD)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_zeros_MV = (df['mark_val_usd'] == 0).sum()\n",
    "print(f\"There are {num_zeros_MV} invalid entries\")\n",
    "\n",
    "df.loc[df['mark_val_usd'] == 0, 'mark_val_usd'] = pd.NA\n",
    "MV_summary = df['mark_val_usd'].describe()\n",
    "print(MV_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the revenue and market value that is is not NA.\n",
    "subset = df[['revenue_usd', 'mark_val_usd']].dropna()\n",
    "X = subset[['revenue_usd']]   # must be 2D\n",
    "y = subset['mark_val_usd']\n",
    "\n",
    "# Fit linear regression\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Regression parameters\n",
    "slope = model.coef_[0]\n",
    "intercept = model.intercept_\n",
    "r_squared = model.score(X, y)\n",
    "\n",
    "# Print regression equation\n",
    "print(f\"Regression equation: Market Value = {slope:.4f} × Revenue + {intercept:.4f}\")\n",
    "print(f\"R² = {r_squared:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, y_pred)\n",
    "plt.xlabel('Revenue (USD)')\n",
    "plt.ylabel('Market Value (USD)')\n",
    "plt.title(f'Market Value vs Revenue')\n",
    "\n",
    "# Uncomment to see only bottom left part. Comment to see everything\n",
    "plt.xlim(0, 1e8)   # zoom into revenue < 100 million\n",
    "plt.ylim(0, 5e8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between `Revenue (USD)` and `IT spend`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = df[(df['revenue_usd'] > 0) & (df['it_spending'] > 0)].dropna(subset=['revenue_usd', 'it_spending'])\n",
    "x = plot_data['it_spending'].values\n",
    "y = plot_data['revenue_usd'].values\n",
    "\n",
    "m, c = np.polyfit(x, y, 1)\n",
    "y_fit = m * x + c\n",
    "R = np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(\n",
    "    x, y,\n",
    "    alpha=0.5, s=30,\n",
    "    edgecolors='white', linewidth=0.5\n",
    ")\n",
    "plt.plot(\n",
    "    x, y_fit,\n",
    "    linewidth=2,\n",
    "    label=f'Best fit line (R = {R:.3f})'\n",
    ")\n",
    "equation_text = f\"Revenue = {m:.2f} × IT spend + {c:.2e}\\nR = {R:.3f}\"\n",
    "\n",
    "plt.text(\n",
    "    0.05, 0.95,\n",
    "    equation_text,\n",
    "    transform=plt.gca().transAxes,\n",
    "    fontsize=11,\n",
    "    verticalalignment='top',\n",
    "    bbox=dict(boxstyle='round', alpha=0.2)\n",
    ")\n",
    "\n",
    "print(f\"{equation_text}\")\n",
    "\n",
    "plt.title('Productivity Analysis: Revenue vs IT Spend')\n",
    "plt.xlabel('IT spend')\n",
    "plt.ylabel('Revenue (USD)')\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Uncomment the bottom 2 lines of code to restrict the x axis to 0-0.5e7 and the y axis to 0-0.2e9.\n",
    "plt.xlim(0, 5e6)\n",
    "plt.ylim(0, 2e8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plotting {len(plot_data)} companies.\")\n",
    "print(f\"Correlation coefficient (R): {R:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, we see that there is a positive (linear) relationship between the revenue of the company and IT spending. However, there seems to exist multiple linear relationships between `Revenue` and `IT spend`, each having its own slope.\n",
    "\n",
    "We can group the above data into 2 categories:\n",
    "- High efficiency firms (high revenue per dollar of IT)\n",
    "- Low efficiency firms (similar IT spend, much lower revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 33.60\n",
    "c = 7.16e5\n",
    "\n",
    "# Note: efficiency column already exists in dummy_working.xlsx\n",
    "# Verify the existing classification\n",
    "plot_data = df[\n",
    "    (df['revenue_usd'] > 0) &\n",
    "    (df['it_spending'] > 0)\n",
    "].dropna(subset=['revenue_usd', 'it_spending', 'efficiency'])\n",
    "\n",
    "color_map = {\n",
    "    'High efficiency': 'seagreen',\n",
    "    'Low efficiency': 'indianred'\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Scatter plot with colour corresponding to responsiveness of revenue (USD) against IT spend (USD)\n",
    "for label, color in color_map.items():\n",
    "    subset = plot_data[plot_data['efficiency'] == label]\n",
    "    \n",
    "    plt.scatter(\n",
    "        subset['it_spending'],\n",
    "        subset['revenue_usd'],\n",
    "        alpha=0.6,\n",
    "        s=30,\n",
    "        color=color,\n",
    "        edgecolors='white',\n",
    "        linewidth=0.5,\n",
    "        label=label\n",
    "    )\n",
    "\n",
    "#Finding out the eqn for the best fit line of revenue against IT spend as well as the R^2 value.\n",
    "equation_text = f\"Revenue = {m:.2f} × IT spend + {c:.2e}\\nR^2 = {R:.3f}\"\n",
    "\n",
    "plt.text(\n",
    "    0.05, 0.95,\n",
    "    equation_text,\n",
    "    transform=plt.gca().transAxes,\n",
    "    fontsize=11,\n",
    "    verticalalignment='top',\n",
    "    bbox=dict(boxstyle='round', alpha=0.2)\n",
    ")\n",
    "\n",
    "\n",
    "# Best-fit line\n",
    "x_vals = np.linspace(0, 5e6, 200)\n",
    "y_vals = m * x_vals + c\n",
    "\n",
    "plt.plot(\n",
    "    x_vals,\n",
    "    y_vals,\n",
    "    color='black',\n",
    "    linestyle='--',\n",
    "    linewidth=2,\n",
    "    label='Best fit line'\n",
    ")\n",
    "\n",
    "plt.title('Productivity Analysis: Revenue vs IT Spend')\n",
    "plt.xlabel('IT spend (USD)')\n",
    "plt.ylabel('Revenue (USD)')\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim(0, 5e6)\n",
    "plt.ylim(0, 2e8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The green points are companies with a high revenue/IT spend ratio and the red points are companies with a lower revenue/IT spend ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between Revenue and Total Employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"revenue_usd\"] = pd.to_numeric(df[\"revenue_usd\"], errors=\"coerce\")\n",
    "df[\"revenue_usd\"] = df[\"revenue_usd\"].replace(0, np.nan)\n",
    "\n",
    "df.dtypes\n",
    "df.head()\n",
    "(df[\"revenue_usd\"] == 0).sum() # Check that the 0's in the revenue are all nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check employees total df\n",
    "df[\"employees_total\"].dtype\n",
    "\n",
    "df[\"employees_total\"] = df[\"employees_total\"].replace(0, np.nan)\n",
    "(df[\"employees_total\"] == 0).sum() # replace 0 with nan for companies with 0 employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only rows where both values exist\n",
    "sub = df[[\"employees_total\", \"revenue_usd\"]].dropna()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(sub[\"employees_total\"], sub[\"revenue_usd\"])\n",
    "plt.xlabel(\"employees_total\")\n",
    "plt.ylabel(\"revenue_usd\")\n",
    "plt.title(\"Revenue (USD) vs Total Employees\")\n",
    "plt.ylim (0, 2e8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sub[\"employees_total\"].corr(sub[\"revenue_usd\"], method=\"pearson\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(sub[\"employees_total\"], sub[\"revenue_usd\"])\n",
    "plt.xlabel(\"employees_total\")\n",
    "plt.ylabel(\"revenue_usd\")\n",
    "plt.title(f\"Revenue (USD) vs Total Employees (R^2 = {r:.3f})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "df[\"company_id\"] = df.index + 1   # 1,2,3,...\n",
    "\n",
    "sub = df[[\"company_id\", \"employees_total\", \"revenue_usd\"]].dropna().copy()\n",
    "sub = sub[(sub[\"employees_total\"] > 0) & (sub[\"revenue_usd\"] > 0)]\n",
    "\n",
    "sub[\"rev_per_emp\"] = sub[\"revenue_usd\"] / sub[\"employees_total\"]\n",
    "\n",
    "sub.sort_values(\"rev_per_emp\", ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(sub[\"employees_total\"], sub[\"rev_per_emp\"])\n",
    "plt.xlabel(\"Employees Total\")\n",
    "plt.ylabel(\"Revenue per Employee\")\n",
    "plt.title(\"Revenue per Employee vs Employees\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `companies_edges.csv` into the files tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pd.read_excel(\"dummy_working.xlsx\")\n",
    "\n",
    "# Fill missing values\n",
    "df.fillna({\n",
    "    \"Id\": \"Unknown\",\n",
    "    \"indegree\": 0,\n",
    "    \"outdegree\": 0,\n",
    "    \"employees_total\": 0,\n",
    "    \"revenue_usd\": 0,\n",
    "    \"sic_desc\": \"NA\",\n",
    "    \"corp_fam_members\": 0,\n",
    "    \"mark_val_usd\": 0,\n",
    "    \"it_spending\": 0,\n",
    "    \"efficiency\": \"Unknown\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Convert data types\n",
    "df = df.astype({\n",
    "    \"Id\": str,\n",
    "    \"indegree\": int,\n",
    "    \"outdegree\": int,\n",
    "    \"employees_total\": int,\n",
    "    \"revenue_usd\": float,\n",
    "    \"sic_desc\": str,\n",
    "    \"corp_fam_members\": int,\n",
    "    \"mark_val_usd\": float,\n",
    "    \"it_spending\": float,\n",
    "    \"efficiency\": str\n",
    "})\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and process company data\n",
    "df = pd.read_excel(\"dummy_working.xlsx\")\n",
    "df.fillna({\"Id\": \"Unknown\", \"indegree\": 0, \"outdegree\": 0,\n",
    "           \"employees_total\": 0, \"revenue_usd\": 0,\n",
    "           \"sic_desc\": \"NA\", \"corp_fam_members\": 0, \"mark_val_usd\": 0,\n",
    "           \"it_spending\": 0, \"efficiency\": \"Unknown\"}, inplace=True)\n",
    "\n",
    "df = df.astype({\"Id\": str, \"indegree\": int, \"outdegree\": int,\n",
    "                \"employees_total\": int, \"revenue_usd\": float,\n",
    "                \"sic_desc\": str, \"corp_fam_members\": int, \"mark_val_usd\": float,\n",
    "                \"it_spending\": float, \"efficiency\": str})\n",
    "\n",
    "edgeList = pd.read_csv(\"companies_edges.csv\", header=None, names=['source', 'target'])\n",
    "\n",
    "# Convert edge list to a set of tuples for O(1) lookup\n",
    "edge_set = set(zip(edgeList['source'], edgeList['target']))\n",
    "\n",
    "# Get list of company IDs\n",
    "company_ids = df[\"Id\"].tolist()\n",
    "\n",
    "# Initialise adjacency matrix with False values\n",
    "adjMat = pd.DataFrame(False, index=company_ids, columns=company_ids)\n",
    "\n",
    "# Populate adjacency matrix\n",
    "for source, target in edge_set:\n",
    "    if target in adjMat.index and source in adjMat.columns:\n",
    "        adjMat.at[target, source] = True\n",
    "\n",
    "print(adjMat.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"dummy_working.xlsx\")\n",
    "\n",
    "pca_df = df.copy()\n",
    "\n",
    "# drop columns not suitable for PCA\n",
    "pca_df = pca_df.drop(columns=[\"Id\", \"sic_desc\"], errors=\"ignore\")\n",
    "\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# log-transform skewed business variables (safe with zeros)\n",
    "skew_cols = [\"employees_total\",\"revenue_usd\",\"corp_fam_members\",\"mark_val_usd\",\"it_spending\"]\n",
    "for c in skew_cols:\n",
    "    if c in pca_df.columns:\n",
    "        pca_df[c] = pd.to_numeric(pca_df[c], errors=\"coerce\").clip(lower=0)\n",
    "        pca_df[c] = np.log1p(pca_df[c])\n",
    "\n",
    "# ensure everything is numeric\n",
    "X = pca_df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# impute missing + scale\n",
    "X_imp = SimpleImputer(strategy=\"median\").fit_transform(X)\n",
    "X_scaled = StandardScaler().fit_transform(X_imp)\n",
    "\n",
    "# PCA\n",
    "pca = PCA()\n",
    "Z = pca.fit_transform(X_scaled)\n",
    "\n",
    "#Calculates how much variance is explained by each successive PC axis.\n",
    "explained = pca.explained_variance_ratio_\n",
    "print(\"Explained variance ratio:\", explained.round(4))\n",
    "print(\"Cumulative:\", explained.cumsum().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % variance per PC\n",
    "var = pca.explained_variance_ratio_ * 100\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(range(1, len(var)+1), var, marker=\"o\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Variance Explained (%)\")\n",
    "plt.title(\"Scree Plot\")\n",
    "plt.xticks(range(1, len(var)+1, 1))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "var = pca.explained_variance_ratio_ * 100  # convert to %\n",
    "\n",
    "print(f\"PC1 variance explained: {var[0]:.2f}%\")\n",
    "print(f\"PC2 variance explained: {var[1]:.2f}%\")\n",
    "print(f\"PC1+PC2 cumulative: {(var[0]+var[1]):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biplot\n",
    "# If you already have Z from fit_transform, you can just use it:\n",
    "pcaX = Z[:, :2]                          # first two principal component SCORES\n",
    "loadings = pca.components_.T[:, :2]      # (n_features, 2)\n",
    "feature_names = X.columns                # feature names used in PCA\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 8))  # Create main figure and axis\n",
    "\n",
    "# Plot data points (scores)\n",
    "sc = ax1.scatter(\n",
    "    pcaX[:, 0], pcaX[:, 1],\n",
    "    s=10, alpha=0.6, linewidth=0.3\n",
    ")\n",
    "\n",
    "# Label your primary axis -> PCA scores\n",
    "ax1.set_xlabel(\"PC1 scores\", fontsize=12, fontweight='bold', labelpad=10)\n",
    "ax1.set_ylabel(\"PC2 scores\", fontsize=12, fontweight='bold', labelpad=10)\n",
    "ax1.set_title(\"PCA Biplot with Score and Loading Axes\", fontsize=14, fontweight='bold', pad=25)\n",
    "ax1.grid(False)\n",
    "\n",
    "# Secondary axis for loadings (fixed to [-1, 1])\n",
    "ax2 = ax1.twinx().twiny()\n",
    "ax2.set_xlim(-1, 1)\n",
    "ax2.set_ylim(-1, 1)\n",
    "ax2.set_xlabel(\"PC1 loadings\", fontsize=12, fontweight='bold', color='red', labelpad=10)\n",
    "ax2.set_ylabel(\"PC2 loadings\", fontsize=12, fontweight='bold', color='red', labelpad=10)\n",
    "ax2.tick_params(axis='x', colors='red')\n",
    "ax2.tick_params(axis='y', colors='red')\n",
    "ax2.grid(False)\n",
    "\n",
    "# Draw loading arrows.\n",
    "# Your scale formula is okay; it makes arrows visible relative to score spread.\n",
    "scale = (np.max(np.abs(pcaX)) / np.max(np.abs(loadings))) * 0.5\n",
    "\n",
    "for feature, (lx, ly) in zip(feature_names, loadings[:, :2]):\n",
    "    ax1.arrow(\n",
    "        0, 0,\n",
    "        lx * scale, ly * scale,\n",
    "        color='red', alpha=0.7,\n",
    "        head_width=0.1,\n",
    "        length_includes_head=True\n",
    "    )\n",
    "    ax1.text(\n",
    "        lx * scale * 1.1, ly * scale * 1.1,\n",
    "        feature,\n",
    "        color='red',\n",
    "        ha='center', va='center',\n",
    "        fontsize=10,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "# Reference axis lines\n",
    "ax1.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "ax1.axvline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Helper: biplot PC1 vs PC2\n",
    "# -----------------------------\n",
    "def biplot_pc1_pc2(Z, loadings, feature_names, title, df, top_n=10):\n",
    "    scores = Z[:, :2]\n",
    "    load2 = loadings[:, :2]\n",
    "    \n",
    "    # efficiency mask + values\n",
    "    mask = df[\"efficiency\"].notna().values\n",
    "    eff = df.loc[mask, \"efficiency\"].map({\n",
    "        \"Low efficiency\": 0,\n",
    "        \"High efficiency\": 1\n",
    "    }).values\n",
    "    \n",
    "    # choose top_n arrows by length (importance in PC1/PC2 plane)\n",
    "    strength = np.sqrt(load2[:, 0]**2 + load2[:, 1]**2)\n",
    "    top_idx = np.argsort(strength)[-top_n:]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    # scatter scores by efficiency\n",
    "    ax.scatter(\n",
    "        scores[mask][eff == 0, 0],\n",
    "        scores[mask][eff == 0, 1],\n",
    "        s=8, alpha=0.35, linewidth=0,\n",
    "        label=\"Low efficiency\"\n",
    "    )\n",
    "    \n",
    "    ax.scatter(\n",
    "        scores[mask][eff == 1, 0],\n",
    "        scores[mask][eff == 1, 1],\n",
    "        s=8, alpha=0.35, linewidth=0,\n",
    "        label=\"High efficiency\"\n",
    "    )\n",
    "    \n",
    "    ax.set_xlabel(\"PC1 scores\", fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"PC2 scores\", fontweight=\"bold\")\n",
    "    ax.set_title(title, fontweight=\"bold\")\n",
    "    ax.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "    ax.axvline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "    ax.grid(False)\n",
    "    \n",
    "    # scale arrows to match the score spread\n",
    "    scale = (np.max(np.abs(scores)) / np.max(np.abs(load2[top_idx]))) * 0.45\n",
    "    \n",
    "    # draw arrows + labels\n",
    "    for i in top_idx:\n",
    "        x, y = load2[i, 0] * scale, load2[i, 1] * scale\n",
    "        ax.arrow(0, 0, x, y, alpha=0.75, head_width=0.06, length_includes_head=True)\n",
    "        ax.text(\n",
    "            x * 1.10, y * 1.10, feature_names[i],\n",
    "            fontsize=9, fontweight=\"bold\",\n",
    "            ha=\"center\", va=\"center\"\n",
    "        )\n",
    "    \n",
    "    ax.legend(frameon=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "biplot_pc1_pc2(\n",
    "    Z, pca.components_.T, X.columns,\n",
    "    title=\"PCA Biplot (PC1 vs PC2) - Colored by Efficiency\",\n",
    "    df=df,\n",
    "    top_n=min(8, len(X.columns))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost and Random Forest Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"dummy_working.xlsx\")\n",
    "\n",
    "df.fillna({\"Id\": \"Unknown\", \"indegree\": 0, \"outdegree\": 0,\n",
    "           \"employees_total\": 0, \"revenue_usd\": 0,\n",
    "           \"sic_desc\": \"NA\", \"corp_fam_members\": 0, \"mark_val_usd\": 0,\n",
    "           \"it_spending\": 0, \"efficiency\": \"Unknown\"}, inplace=True)\n",
    "\n",
    "df = df.astype({\"Id\": str, \"indegree\": int, \"outdegree\": int,\n",
    "                \"employees_total\": int, \"revenue_usd\": float,\n",
    "                \"sic_desc\": str, \"corp_fam_members\": int, \"mark_val_usd\": float,\n",
    "                \"it_spending\": float, \"efficiency\": str})\n",
    "\n",
    "print(f\"Original dataframe shape: {df.shape}\")\n",
    "if df.duplicated(subset=['Id']).any():\n",
    "    print(f\"WARNING: Found {df.duplicated(subset=['Id']).sum()} duplicate company IDs\")\n",
    "    df = df.drop_duplicates(subset=['Id'], keep='first')\n",
    "    print(f\"After removing duplicates: {df.shape}\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['sic_desc_encoded'] = le.fit_transform(df['sic_desc'])\n",
    "\n",
    "df = df.set_index(\"Id\")\n",
    "\n",
    "edgeList = pd.read_csv(\"companies_edges.csv\", header=None, names=['source', 'target'])\n",
    "edge_set = set(zip(edgeList['source'], edgeList['target']))\n",
    "\n",
    "print(f\"Total companies: {len(df)}\")\n",
    "print(f\"Total edges: {len(edge_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairwise_dataset_balanced(attributes, edge_set, negative_multiplier=2):\n",
    "    # Select only numeric columns\n",
    "    features_to_use = attributes.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    print(f\"Using {len(features_to_use)} numeric features\")\n",
    "    \n",
    "    # Convert to numpy for fast access\n",
    "    company_ids = attributes.index.tolist()\n",
    "    company_to_idx = {cid: idx for idx, cid in enumerate(company_ids)}\n",
    "    attributes_array = attributes[features_to_use].values\n",
    "    \n",
    "    # Separate positive and negative pairs\n",
    "    positive_pairs = []\n",
    "    all_companies_set = set(company_ids)\n",
    "    \n",
    "    print(\"Processing positive pairs (actual acquisitions)...\")\n",
    "    for source, target in edge_set:\n",
    "        if source in all_companies_set and target in all_companies_set:\n",
    "            parent_idx = company_to_idx[target]\n",
    "            child_idx = company_to_idx[source]\n",
    "            \n",
    "            parent_attrs = attributes_array[parent_idx]\n",
    "            child_attrs = attributes_array[child_idx]\n",
    "            \n",
    "            combined = np.concatenate([\n",
    "                parent_attrs,\n",
    "                child_attrs,\n",
    "                parent_attrs - child_attrs,\n",
    "                parent_attrs / (child_attrs + 1e-8),\n",
    "            ])\n",
    "            \n",
    "            positive_pairs.append(combined)\n",
    "    \n",
    "    print(f\"Found {len(positive_pairs)} positive pairs\")\n",
    "    \n",
    "    # Sample negative pairs\n",
    "    num_negatives_needed = len(positive_pairs) * negative_multiplier\n",
    "    print(f\"Sampling {num_negatives_needed} negative pairs...\")\n",
    "    \n",
    "    negative_pairs = []\n",
    "    edge_dict = {(target, source) for source, target in edge_set}\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    attempts = 0\n",
    "    max_attempts = num_negatives_needed * 10  # Safety limit\n",
    "    \n",
    "    while len(negative_pairs) < num_negatives_needed and attempts < max_attempts:\n",
    "        # Randomly sample two different companies\n",
    "        parent_id = np.random.choice(company_ids)\n",
    "        child_id = np.random.choice(company_ids)\n",
    "        \n",
    "        # Skip if same company or if this is actually a positive pair\n",
    "        if parent_id == child_id or (parent_id, child_id) in edge_dict:\n",
    "            attempts += 1\n",
    "            continue\n",
    "        \n",
    "        parent_idx = company_to_idx[parent_id]\n",
    "        child_idx = company_to_idx[child_id]\n",
    "        \n",
    "        parent_attrs = attributes_array[parent_idx]\n",
    "        child_attrs = attributes_array[child_idx]\n",
    "        \n",
    "        combined = np.concatenate([\n",
    "            parent_attrs,\n",
    "            child_attrs,\n",
    "            parent_attrs - child_attrs,\n",
    "            parent_attrs / (child_attrs + 1e-8),\n",
    "        ])\n",
    "        \n",
    "        negative_pairs.append(combined)\n",
    "        attempts += 1\n",
    "        \n",
    "        if len(negative_pairs) % 500 == 0:\n",
    "            print(f\"  Sampled {len(negative_pairs)}/{num_negatives_needed} negative pairs...\")\n",
    "    \n",
    "    # Combine positive and negative\n",
    "    X = np.vstack([positive_pairs, negative_pairs])\n",
    "    y = np.array([1] * len(positive_pairs) + [0] * len(negative_pairs))\n",
    "    \n",
    "    print(f\"\\nFinal dataset:\")\n",
    "    print(f\"  Total pairs: {len(y):,}\")\n",
    "    print(f\"  Positive: {sum(y):,} ({sum(y)/len(y)*100:.1f}%)\")\n",
    "    print(f\"  Negative: {len(y) - sum(y):,} ({(len(y)-sum(y))/len(y)*100:.1f}%)\")\n",
    "    print(f\"  Feature vector size: {X.shape[1]}\")\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create balanced dataset\n",
    "X, y = create_pairwise_dataset_balanced(df, edge_set, negative_multiplier=2)\n",
    "# negative_multiplier means 2 negative samples per positive sample\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train):,} samples\")\n",
    "print(f\"Test set: {len(X_test):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = (len(y_train) - sum(y_train)) / sum(y_train)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'gamma': [0, 0.1, 0.2], # Regularization: helps prune branches that don't help\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "xgb_base = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    scale_pos_weight=ratio, # Keep the ratio we calculated earlier\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='auc',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_base,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20, # n_iter=20 means it will try 20 different random combinations\n",
    "    scoring='roc_auc',\n",
    "    cv=5, # 5-fold cross validation\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1 # Use all CPU cores\n",
    ")\n",
    "\n",
    "print(\"Starting Parameter Search...\")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "print(f\"\\nBest Parameters: {random_search.best_params_}\")\n",
    "print(f\"Best CV Score (AUC): {random_search.best_score_:.4f}\")\n",
    "\n",
    "feature_names = []\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "for prefix in ['parent_', 'child_', 'diff_', 'ratio_']:\n",
    "    feature_names.extend([prefix + f for f in numeric_features])\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': best_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Random Forest Model...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "def evaluate(model, name):\n",
    "    probs = model.predict_proba(X_test)[:, 1]\n",
    "    preds = model.predict(X_test)\n",
    "    print(f\"\\n--- {name} Results ---\")\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, probs):.4f}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, preds):.4f}\")\n",
    "    print(classification_report(y_test, preds, target_names=['No Deal', 'Acquisition']))\n",
    "\n",
    "evaluate(rf_model, \"Random Forest\")\n",
    "\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(rf_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Loading and Preprocessing\n",
    "def load_and_preprocess_data(nodes_path, edges_path):\n",
    "    if not os.path.exists(nodes_path) or not os.path.exists(edges_path):\n",
    "        raise FileNotFoundError(\"Check your file paths; one or both files are missing.\")\n",
    "    \n",
    "    print(f\"Loading nodes from {nodes_path}...\")\n",
    "    # Load Excel file (ensure openpyxl is installed)\n",
    "    nodes_df = pd.read_excel(nodes_path)\n",
    "    \n",
    "    # --- RECTIFICATION FOR DUPLICATE INDEX ERROR ---\n",
    "    initial_count = len(nodes_df)\n",
    "    nodes_df = nodes_df.drop_duplicates(subset=['Id'], keep='first')\n",
    "    final_count = len(nodes_df)\n",
    "    if initial_count > final_count:\n",
    "        print(f\"Removed {initial_count - final_count} duplicate company IDs.\")\n",
    "    # -----------------------------------------------\n",
    "    \n",
    "    print(f\"Loading edges from {edges_path}...\")\n",
    "    # Read CSV with latin1 to handle special characters from Excel exports\n",
    "    edges_df = pd.read_csv(edges_path, encoding='latin1')\n",
    "    \n",
    "    # Preprocess nodes (Encoding Categoricals)\n",
    "    le_sic = LabelEncoder()\n",
    "    nodes_df['sic_desc'] = le_sic.fit_transform(nodes_df['sic_desc'].astype(str))\n",
    "    \n",
    "    le_eff = LabelEncoder()\n",
    "    nodes_df['efficiency'] = le_eff.fit_transform(nodes_df['efficiency'].astype(str))\n",
    "    \n",
    "    # Scale numerical features (Essential for RL convergence)\n",
    "    num_cols = ['indegree', 'outdegree', 'employees_total', 'revenue_usd',\n",
    "                'corp_fam_members', 'mark_val_usd', 'it_spending']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    nodes_df[num_cols] = scaler.fit_transform(nodes_df[num_cols])\n",
    "    \n",
    "    # Prepare feature lookup\n",
    "    feature_cols = num_cols + ['sic_desc', 'efficiency']\n",
    "    node_features = nodes_df.set_index('Id')[feature_cols].to_dict('index')\n",
    "    \n",
    "    # Prepare label lookup (Adjacency Set)\n",
    "    adj_set = set(zip(edges_df['source'].astype(str), edges_df['target'].astype(str)))\n",
    "    company_ids = nodes_df['Id'].astype(str).tolist()\n",
    "    \n",
    "    return node_features, adj_set, company_ids, len(feature_cols)\n",
    "\n",
    "# 2. Custom Gymnasium Environment\n",
    "class CompanyAcquisitionEnv(gym.Env):\n",
    "    def __init__(self, node_features, adj_set, company_ids, feature_dim):\n",
    "        super(CompanyAcquisitionEnv, self).__init__()\n",
    "        self.node_features = node_features\n",
    "        self.adj_set = adj_set\n",
    "        self.company_ids = company_ids\n",
    "        self.feature_dim = feature_dim\n",
    "        \n",
    "        # Action: 0 (No), 1 (Yes)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        # State: Concatenated Acquirer + Target features\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(2 * self.feature_dim,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.current_acquirer = None\n",
    "        self.current_target = None\n",
    "        self.steps_per_episode = 100\n",
    "        self.current_step = 0\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        # Retrieve features from dictionaries\n",
    "        acq_feat = np.array(list(self.node_features[self.current_acquirer].values()))\n",
    "        trg_feat = np.array(list(self.node_features[self.current_target].values()))\n",
    "        return np.concatenate([acq_feat, trg_feat]).astype(np.float32)\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Pick random companies for the start of the episode\n",
    "        self.current_acquirer = np.random.choice(self.company_ids)\n",
    "        self.current_target = np.random.choice(self.company_ids)\n",
    "        \n",
    "        # Prevent self-acquisition\n",
    "        while self.current_target == self.current_acquirer:\n",
    "            self.current_target = np.random.choice(self.company_ids)\n",
    "        \n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        is_true_acquisition = (self.current_acquirer, self.current_target) in self.adj_set\n",
    "        \n",
    "        if action == 1: # Predict \"Acquire\"\n",
    "            if is_true_acquisition:\n",
    "                reward = 10.0  # High reward for finding the needle in the haystack\n",
    "            else:\n",
    "                reward = -1.0  # Penalty for false positive\n",
    "        else: # Predict \"Skip\"\n",
    "            if is_true_acquisition:\n",
    "                reward = -5.0  # Significant penalty for missing a real opportunity\n",
    "            else:\n",
    "                reward = 0.1   # Small reward for correct rejection\n",
    "        \n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Transition to next random pair\n",
    "        self.current_acquirer = np.random.choice(self.company_ids)\n",
    "        self.current_target = np.random.choice(self.company_ids)\n",
    "        while self.current_target == self.current_acquirer:\n",
    "            self.current_target = np.random.choice(self.company_ids)\n",
    "        \n",
    "        terminated = self.current_step >= self.steps_per_episode\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    NODES_FILE = 'dummy_working.xlsx'\n",
    "    EDGES_FILE = 'companies_edges.csv'\n",
    "    \n",
    "    try:\n",
    "        print(\"Initializing data processing...\")\n",
    "        node_feats, adj_set, ids, f_dim = load_and_preprocess_data(NODES_FILE, EDGES_FILE)\n",
    "        \n",
    "        # Instantiate Environment\n",
    "        env = CompanyAcquisitionEnv(node_feats, adj_set, ids, f_dim)\n",
    "        check_env(env)\n",
    "        \n",
    "        # Setup and Train PPO Model\n",
    "        print(\"Training model...\")\n",
    "        model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "        model.learn(total_timesteps=30000)\n",
    "        \n",
    "        # Save Model\n",
    "        model.save(\"acquisition_model_final\")\n",
    "        print(\"\\nSuccess! Model saved as 'acquisition_model_final.zip'\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Process failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "nodes_df = pd.read_excel('dummy_working.xlsx')\n",
    "edges_df = pd.read_csv('companies_edges.csv')\n",
    "\n",
    "# This ensures no duplicates\n",
    "nodes_df = nodes_df.drop_duplicates(subset=['Id']).reset_index(drop=True)\n",
    "\n",
    "num_cols = ['indegree', 'outdegree', 'employees_total', 'revenue_usd',\n",
    "            'corp_fam_members', 'mark_val_usd', 'it_spending']\n",
    "scaler = StandardScaler()\n",
    "nodes_df[num_cols] = scaler.fit_transform(nodes_df[num_cols])\n",
    "\n",
    "sic_dummies = pd.get_dummies(nodes_df['sic_desc'], prefix='sic')\n",
    "eff_dummies = pd.get_dummies(nodes_df['efficiency'], prefix='eff')\n",
    "\n",
    "# Combine features and store column names for the interface\n",
    "X_df = pd.concat([nodes_df[num_cols], sic_dummies, eff_dummies], axis=1).astype(float)\n",
    "feature_cols = X_df.columns.tolist()\n",
    "X = torch.tensor(X_df.values, dtype=torch.float)\n",
    "\n",
    "# Mapping node IDs to indices\n",
    "node_to_idx = {name: i for i, name in enumerate(nodes_df['Id'])}\n",
    "idx_to_node = {i: name for name, i in node_to_idx.items()}\n",
    "num_nodes = len(node_to_idx)\n",
    "\n",
    "# Filter edges to only include nodes present in the cleaned nodes list\n",
    "valid_edges = edges_df[edges_df['source'].isin(node_to_idx) &\n",
    "                      edges_df['target'].isin(node_to_idx)]\n",
    "edge_indices = [[node_to_idx[r['source']], node_to_idx[r['target']]] for _, r in valid_edges.iterrows()]\n",
    "edge_index = torch.tensor(edge_indices, dtype=torch.long).t()\n",
    "\n",
    "# 2. Define Model Structure\n",
    "class GCNLinkPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCNLinkPredictor, self).__init__()\n",
    "        self.lin1 = nn.Linear(in_channels, hidden_channels)\n",
    "        self.lin2 = nn.Linear(hidden_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        # Sparse Matrix Multiplication for GCN layer\n",
    "        x = F.relu(self.lin1(torch.spmm(adj, x)))\n",
    "        x = self.lin2(torch.spmm(adj, x))\n",
    "        return x\n",
    "\n",
    "def get_adj_norm(edge_index, num_nodes):\n",
    "    row, col = edge_index.numpy()\n",
    "    adj = coo_matrix((np.ones(row.shape[0]), (row, col)), shape=(num_nodes, num_nodes))\n",
    "    adj = (adj + adj.T).tocoo()\n",
    "    adj.setdiag(1)\n",
    "    d_inv_sqrt = np.power(np.array(adj.sum(1)), -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat = coo_matrix((d_inv_sqrt, (np.arange(num_nodes), np.arange(num_nodes))))\n",
    "    adj_norm = d_mat.dot(adj).dot(d_mat).tocoo()\n",
    "    indices = torch.from_numpy(np.vstack((adj_norm.row, adj_norm.col)).astype(np.int64))\n",
    "    return torch.sparse_coo_tensor(indices, torch.from_numpy(adj_norm.data.astype(np.float32)), [num_nodes, num_nodes])\n",
    "\n",
    "# 3. Train the Model\n",
    "adj = get_adj_norm(edge_index, num_nodes)\n",
    "model = GCNLinkPredictor(X.shape[1], 32, 16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(101):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model(X, adj)\n",
    "    pos_scores = (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n",
    "    neg_idx = torch.randint(0, num_nodes, (2, edge_index.shape[1]))\n",
    "    neg_scores = (z[neg_idx[0]] * z[neg_idx[1]]).sum(dim=-1)\n",
    "    loss = -torch.log(torch.sigmoid(pos_scores) + 1e-15).mean() - \\\n",
    "           torch.log(1 - torch.sigmoid(neg_scores) + 1e-15).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 4. Generate Model Files\n",
    "torch.save(model.state_dict(), 'gnn_acquisition_model.pth')\n",
    "metadata = {\n",
    "    'node_to_idx': node_to_idx,\n",
    "    'idx_to_node': idx_to_node,\n",
    "    'num_cols': num_cols,\n",
    "    'feature_cols': feature_cols,\n",
    "    'scaler': scaler,\n",
    "    'in_channels': X.shape[1],\n",
    "    'hidden_channels': 32,\n",
    "    'out_channels': 16\n",
    "}\n",
    "with open('model_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(\"Saved: gnn_acquisition_model.pth and model_metadata.pkl\")\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "def evaluate(model, X, adj, pos_edges, neg_edges):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(X, adj)\n",
    "        \n",
    "        pos_scores = (z[pos_edges[0]] * z[pos_edges[1]]).sum(dim=-1)\n",
    "        neg_scores = (z[neg_edges[0]] * z[neg_edges[1]]).sum(dim=-1)\n",
    "        \n",
    "        scores = torch.cat([pos_scores, neg_scores]).cpu().numpy()\n",
    "        labels = np.hstack([\n",
    "            np.ones(len(pos_scores)),\n",
    "            np.zeros(len(neg_scores))\n",
    "        ])\n",
    "        \n",
    "        probs = torch.sigmoid(torch.tensor(scores)).numpy()\n",
    "        \n",
    "        auc = roc_auc_score(labels, probs)\n",
    "        ap = average_precision_score(labels, probs)\n",
    "    \n",
    "    return auc, ap\n",
    "\n",
    "def split_edges(edge_index, val_ratio=0.1, test_ratio=0.1):\n",
    "    num_edges = edge_index.shape[1]\n",
    "    perm = torch.randperm(num_edges)\n",
    "    \n",
    "    test_size = int(num_edges * test_ratio)\n",
    "    val_size = int(num_edges * val_ratio)\n",
    "    \n",
    "    test_edges = edge_index[:, perm[:test_size]]\n",
    "    val_edges = edge_index[:, perm[test_size:test_size+val_size]]\n",
    "    train_edges = edge_index[:, perm[test_size+val_size:]]\n",
    "    \n",
    "    return train_edges, val_edges, test_edges\n",
    "\n",
    "def negative_sampling(num_nodes, num_samples):\n",
    "    return torch.randint(0, num_nodes, (2, num_samples))\n",
    "\n",
    "train_e, val_e, test_e = split_edges(edge_index)\n",
    "\n",
    "val_neg = negative_sampling(num_nodes, val_e.shape[1])\n",
    "test_neg = negative_sampling(num_nodes, test_e.shape[1])\n",
    "\n",
    "val_auc, val_ap = evaluate(model, X, adj, val_e, val_neg)\n",
    "test_auc, test_ap = evaluate(model, X, adj, test_e, test_neg)\n",
    "\n",
    "print(f\"Validation AUC: {val_auc:.4f}, AP: {val_ap:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}, AP: {test_ap:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
